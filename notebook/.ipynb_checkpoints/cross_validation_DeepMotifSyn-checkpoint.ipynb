{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import mat4py\n",
    "from sklearn import metrics\n",
    "import logomaker\n",
    "from sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay\n",
    "import seqlogo\n",
    "import scipy\n",
    "import re\n",
    "from matplotlib import gridspec\n",
    "import scipy\n",
    "from sklearn.metrics import auc, average_precision_score\n",
    "from collections import OrderedDict\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import joblib\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import pickle as pkl\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import os\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import auc, average_precision_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(666)\n",
    "torch.set_deterministic(True)\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepMotifSyn generator\n",
    "class deeper_u_net(nn.Module):\n",
    "    def __init__(self,device='cuda'):\n",
    "        super(deeper_u_net, self).__init__()\n",
    "        self.device = device\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 108, out_channels = 64, kernel_size = 4, stride = 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size = 2, stride = 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels = 128, out_channels = 64, kernel_size = 2, stride = 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=64*2, out_channels=64, kernel_size = 4, stride = 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.bottleneck = torch.nn.Sequential(\n",
    "                            torch.nn.Conv1d(kernel_size=3, in_channels=128, out_channels=256, stride=1),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.BatchNorm1d(256),\n",
    "                            torch.nn.Conv1d(kernel_size=1, in_channels=256, out_channels=256, stride=1),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.BatchNorm1d(256),\n",
    "                            torch.nn.ConvTranspose1d(in_channels=256, out_channels=128, kernel_size=3, stride=1)\n",
    "                            )\n",
    "        \n",
    "        self.cnn_out = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64+8, out_channels=32, kernel_size=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=32, out_channels=16, kernel_size=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=16, out_channels=4, kernel_size=1),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        seq_x = x[:,:,:8]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        en1 = self.encoder1(x)\n",
    "        en2 = self.encoder2(en1)\n",
    "        x = self.bottleneck(en2)\n",
    "\n",
    "        x = self.decoder1(x)\n",
    "        x = self.decoder2(torch.cat((x, en1), 1))\n",
    "        \n",
    "        seq_x = seq_x.permute(0 ,2, 1)\n",
    "\n",
    "        x = torch.cat((x, seq_x), 1)\n",
    "        out = self.cnn_out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_MSE(input_seq, label_seq):\n",
    "    weight = ((label_seq != -1)*1.0).to(dtype=torch.float32)\n",
    "    valid_len = weight.sum()/4\n",
    "    dist = (input_seq-label_seq)**2\n",
    "    w_dist = dist*weight\n",
    "    mean_dist = w_dist.sum()/valid_len\n",
    "    return mean_dist\n",
    "\n",
    "def weighted_EuclideanDist(input_seq, label_seq):\n",
    "    weight = ((label_seq != -1)*1.0).to(dtype=torch.float32)\n",
    "    valid_len = weight.sum()/4\n",
    "    dist = (input_seq-label_seq)**2\n",
    "    w_dist = dist*weight\n",
    "    w_dist = torch.sqrt(w_dist.sum(0)).sum()\n",
    "    mean_dist = w_dist/valid_len\n",
    "    return mean_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model, epoch_i):\n",
    "        score = val_loss\n",
    "        self.save_checkpoint(val_loss, model, epoch_i)\n",
    "        \"\"\"\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, epoch_i)\n",
    "        elif score > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, epoch_i)\n",
    "            self.counter = 0\n",
    "        \"\"\"\n",
    "        \n",
    "    def save_checkpoint(self, val_loss, model, epoch_i):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save({\n",
    "            'epoch': epoch_i,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss,\n",
    "            'whole_model': model}, \n",
    "            self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "        # print(self.path)\n",
    "        \n",
    "\n",
    "def train(net, train_ds, fold_i, save_folder, model_name = 'tmp' ):\n",
    "    X_tv, y_tv = train_ds\n",
    "    X_train, X_valid = X_tv, X_tv\n",
    "    y_train, y_valid = y_tv, y_tv\n",
    "    # X_train, y_train = train_ds\n",
    "    \n",
    "    train = data_utils.TensorDataset(torch.from_numpy(X_train),torch.from_numpy(y_train))\n",
    "    trainloader = data_utils.DataLoader(train, batch_size=100, shuffle=True, num_workers = 1)\n",
    "    early_stopping = EarlyStopping(patience=7, \n",
    "                verbose=True, path= save_folder + \"/fold_\" + str(fold_i) + \"_best_\"+model_name+\"_checkpoint.pt\")\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    for epoch in tqdm.tqdm(range(200)):\n",
    "        running_loss = 0\n",
    "        for data in trainloader:\n",
    "            input_seq = data[0].to(device, non_blocking=True).float()\n",
    "            label_seq = data[1].to(device, non_blocking=True).float()\n",
    "            # input_family = data[2].to(device, non_blocking=True).float()\n",
    "            # print(inputs.size())\n",
    "            optimizer.zero_grad()\n",
    "            output = net(input_seq)\n",
    "            label_seq = label_seq.permute(0, 2, 1).contiguous()\n",
    "            loss = weighted_MSE(output, label_seq)\n",
    "            # propagate the loss backward\n",
    "            loss.backward()\n",
    "            # update the gradients\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "    val_mse = validate(net, [X_valid, y_valid])\n",
    "    early_stopping(val_mse, net, epoch)\n",
    "    \n",
    "def validate(net, valid_ds):\n",
    "    from scipy import stats\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    X_valid, y_valid = valid_ds\n",
    "    valid = data_utils.TensorDataset(torch.from_numpy(X_valid),torch.from_numpy(y_valid))\n",
    "    validloader = data_utils.DataLoader(valid, batch_size=len(X_valid))\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            input_seq = data[0].to(device, non_blocking=True).float()\n",
    "            label_seq = data[1].to(device, non_blocking=True).float()\n",
    "            # input_family = data[2].to(device, non_blocking=True).float()\n",
    "            label_seq = label_seq.permute(0, 2, 1).contiguous()\n",
    "            output = net(input_seq)\n",
    "            # mse_loss = nn.MSELoss()\n",
    "            loss = weighted_MSE(output, label_seq)\n",
    "        \n",
    "    # slope, intercept, r_value, p_value, std_err = stats.linregress(preds, actual)\n",
    "    print(\"valid: MSE\", loss.item())\n",
    "    return loss.item()\n",
    "\n",
    "def test(net, test_ds, fold_i, save_folder, model_name='tmp'):\n",
    "    from scipy import stats\n",
    "    X_test, y_test = test_ds\n",
    "    test = data_utils.TensorDataset(torch.from_numpy(X_test),torch.from_numpy(y_test))\n",
    "    \n",
    "    testloader = data_utils.DataLoader(test, batch_size=len(X_test))\n",
    "    checkpoint = torch.load(save_folder + \"/fold_\" + str(fold_i) + \"_best_\"+model_name+\"_checkpoint.pt\")\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Load the best model from fold_\" + str(fold_i) + \"_best_\"+model_name+\"_checkpoint.pt\")\n",
    "    net.eval()\n",
    "    weight_mse_loss = []\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        actual = []\n",
    "        # combine_preds = []\n",
    "        for data in testloader:\n",
    "            input_seq = data[0].to(device, non_blocking=True).float()\n",
    "            label_seq = data[1].to(device, non_blocking=True).float()\n",
    "            # input_family = data[2].to(device, non_blocking=True).float()\n",
    "            # X_naive_combine = ((input_seq[:,:,:4] + input_seq[:,:,4:8])/2).permute(0, 2, 1)\n",
    "            \n",
    "            label_seq = label_seq.permute(0, 2, 1).contiguous()\n",
    "            output = net(input_seq)\n",
    "            weight_mse_loss.append(weighted_EuclideanDist(output, label_seq).item())\n",
    "            outputs = list(output.cpu().detach().numpy())\n",
    "            labels = list(label_seq.cpu().detach().numpy())\n",
    "            preds += outputs\n",
    "            actual += labels\n",
    "            # combine_preds += X_naive_combine\n",
    "    \n",
    "    fold_res = []\n",
    "    # fold_naive_res = []\n",
    "    for i in range(len(preds)):\n",
    "        pred_dimer = preds[i]\n",
    "        true_dimer = actual[i]\n",
    "        # combine_dimer = combine_preds[i]\n",
    "        fold_res.append(weighted_EuclideanDist_numpy(pred_dimer, true_dimer))\n",
    "        # fold_naive_res.append(weighted_EuclideanDist_numpy(combine_dimer.cpu().numpy(), true_dimer))\n",
    "    mses = np.array(fold_res).flatten()\n",
    "    # naive_mses = np.array(fold_naive_res).flatten()\n",
    "    print('Euclidean Distance Error', fold_i, np.mean(mses))\n",
    "    if not os.path.exists(save_folder+\"/predictions/\"):\n",
    "        os.mkdir(save_folder+\"/predictions/\")\n",
    "        \n",
    "    save_nparr = np.array([preds, actual])\n",
    "    # print(save_nparr.shape)\n",
    "    # pkl.dump([preds, actual], open(save_folder + \"/predictions/\" + str(fold_i) + \"_\"+ model_name + \"_predictions.pkl\", \"wb\"))\n",
    "    np.save(save_folder + \"/predictions/\" + str(fold_i) + \"_\"+ model_name + \"_predictions.np\", save_nparr)\n",
    "    return mses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave-one-motifpair-out cross-validation of DeepMotifSyn Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tatol number of motif pair: 313\n",
      "-------------------- DNA-binding Family: ETS_BZIP | Testing motif list: ['ETV2_TEF' 'ETV2_TEF_2'] --------------------\n",
      "#Train_motifs 612 #Test_motifs 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [02:20<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: MSE 0.010088411159813404\n",
      "Validation loss decreased (inf --> 0.010088).  Saving model ...\n",
      "Load the best model from fold_ETS_BZIP-ETV2_TEF_best_u_net_checkpoint.pt\n",
      "Euclidean Distance Error ETS_BZIP-ETV2_TEF 0.15714800783781233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(1)\n",
    "device = 'cuda'\n",
    "found_mp_name, found_mp_family, _, true_mp_code, found_mp_dimer_code = pkl.load(open(\"../data/found_best_aligned_mp_allFam_correctedFamilyName.pkl\", \"rb\"))\n",
    "aligned_motifpairs = true_mp_code\n",
    "family_names = found_mp_family\n",
    "dimer_names = found_mp_name\n",
    "dimer_codes = found_mp_dimer_code\n",
    "\n",
    "unique_dnames = []\n",
    "for dn in dimer_names:\n",
    "    unique_dnames.append(\"_\".join(dn.split(\"_\")[:2]))\n",
    "\n",
    "unique_dnames = np.array(unique_dnames)\n",
    "print(\"Tatol number of motif pair:\", len(set(unique_dnames)))\n",
    "\n",
    "# loo_folder = \"./leave.KConeDimer.out.uNetAdv.crxvalidate/\"\n",
    "family_names = np.array(family_names)\n",
    "dimer_names = np.array(dimer_names)\n",
    "save_folder = \"../leave.one.motifpair.out.crxvalidate.generator/\"\n",
    "# save_folder = \"./leave.one.motifpair.out.uNetAdv.crxvalidate/\"\n",
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)\n",
    "\n",
    "# for train_index, test_index in loo.split(unique_dnames):\n",
    "n = 0\n",
    "for fami, udname in enumerate(set(unique_dnames)):\n",
    "    test_index = unique_dnames == udname\n",
    "    train_index = unique_dnames != udname\n",
    "    \n",
    "    test_family_name = family_names[test_index]\n",
    "    test_dimer_name = dimer_names[test_index]\n",
    "    test_unique_dname = unique_dnames[test_index]\n",
    "    \n",
    "    reversed_fam = test_family_name[0].split(\"_\")[1] + \"_\" + test_family_name[0].split(\"_\")[0]\n",
    "    tune_index = np.arange(len(dimer_names))[np.array(family_names == test_family_name[0])*np.array(dimer_names != test_dimer_name[0])]\n",
    "    tune_index = np.concatenate([tune_index, np.arange(len(dimer_names))[np.array(family_names == reversed_fam)*np.array(dimer_names != test_dimer_name[0])]])\n",
    "\n",
    "    fold_name = test_family_name[0] + \"-\"+ udname\n",
    "    \n",
    "    print(\"-\"*20, \"DNA-binding Family:\", test_family_name[0],\"|\",\"Testing motif list:\", test_dimer_name, \"-\"*20)\n",
    "    # net = Autoencoder_seq2seq().to(device)\n",
    "    net = deeper_u_net().to(device)\n",
    "    # net = torch.nn.DataParallel(model, device_ids=[0])\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    \n",
    "    \n",
    "    X_train = aligned_motifpairs[train_index]\n",
    "    X_tune = aligned_motifpairs[tune_index]\n",
    "    X_test = aligned_motifpairs[test_index]\n",
    "    y_train = dimer_codes[train_index]\n",
    "    y_tune = dimer_codes[tune_index]\n",
    "    y_test = dimer_codes[test_index]\n",
    "    \n",
    "    train_name = dimer_names[train_index]\n",
    "    test_name = dimer_names[test_index]\n",
    "    tune_name = dimer_names[tune_index]\n",
    "    \n",
    "    print(\"#Train_motifs\", len(X_train), \"#Test_motifs\", len(X_test))\n",
    "    \n",
    "    model_name='u_net'\n",
    "    train(net, [X_train, y_train], fold_name, model_name=model_name, save_folder=save_folder)\n",
    "    test(net, [X_test, y_test], fold_name, model_name=model_name, save_folder=save_folder)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduce paper figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave-one-motifpair-out cross-validation of DeepMotifSyn evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnerated_motif_df = pd.read_csv(\"../data/generated_motifpairs_with_label_rmDuplicates.csv\")\n",
    "mp_replaced_seqs = np.load(\"../data/deeper_uNet_loMPo_predictive_all_possible_alignedMP_dedup.npy\")\n",
    "features_784 = np.load(\"../data/all_generative_dimer_784features_dedup_correctedFam.npy\")\n",
    "mp_replaced_seqs = np.array(mp_replaced_seqs)\n",
    "labels = np.array(gnerated_motif_df['label'])\n",
    "binary_labels = labels>0\n",
    "mp_replaced_seqs = mp_replaced_seqs.reshape(368995, -1)\n",
    "seqWithNonSeq =  np.concatenate([features_784, mp_replaced_seqs], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"../leave.one.motifpair.out.crxvalidate.evaluator/\"\n",
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368995\n",
      "---------- 0 ETV2_TEF ----------\n",
      "(367891, 924) (1104,)\n",
      "[19:08:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "generated_motif_df = pd.read_csv(\"../data/generated_motifpairs_with_label_rmDuplicates.csv\")\n",
    "print(len(generated_motif_df))\n",
    "# fig, ax = plt.subplots()\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "fold_preds = {}\n",
    "dimer_names = generated_motif_df['dimer_name']\n",
    "for fold_i, dn in enumerate(set(dimer_names)):        \n",
    "    print(\"-\"*10, fold_i, dn, \"-\"*10)\n",
    "    train_index = np.arange(len(dimer_names))[dimer_names != dn]\n",
    "    test_index = np.arange(len(dimer_names))[dimer_names == dn]\n",
    "    \n",
    "    X_train = seqWithNonSeq[train_index]\n",
    "    X_test = seqWithNonSeq[test_index]\n",
    "    y_train = binary_labels[train_index]\n",
    "    y_test = binary_labels[test_index]\n",
    "    print('#Train:', train.shape,'#Test:', y_test.shape)\n",
    "    generated_motif_df.loc[test_index, 'test_dimer_name'] = dn\n",
    "    \n",
    "    # classifier\n",
    "    xgb = XGBClassifier(subsample=1.0, n_estimators=200, min_child_weight=1, max_depth=5, learning_rate=0.0525, gamma=5, colsample_bytree=0.8, n_jobs=20)\n",
    "    \n",
    "    # start_time = time.time()\n",
    "    xgb.fit(X_train, y_train)\n",
    "    y_pred = rf.predict_proba(X_test)\n",
    "    generated_motif_df.loc[test_index, 'xgboost_prediction'] = y_pred[:,1]\n",
    "    # dump(xgb, './fold_' + dn + \"_XGBoost_bestHyper_924features.joblib\")\n",
    "    # print('model saved!')\n",
    "    # print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    fold_preds[dn] = np.array(list(zip(y_pred[:,1], y_test, test_index)))\n",
    "\n",
    "    fpr, tpr, thresholds_roc = metrics.roc_curve(y_test, y_pred[:,1])\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    average_precision=average_precision_score(y_test, y_pred[:,1])\n",
    "    print(roc_auc, average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
